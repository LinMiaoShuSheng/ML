{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#设置多种优化器，根据参数选择\n",
    "with tf.variable_scope(\"optimizer\"):\n",
    "    optimizer = self.config[\"optimizer\"]\n",
    "    if optimizer == \"sgd\":\n",
    "        self.opt = tf.train.GradientDescentOptimizer(self.lr)\n",
    "    elif optimizer == \"adam\":\n",
    "        self.opt = tf.train.AdamOptimizer(self.lr)\n",
    "    elif optimizer == \"adgrad\":\n",
    "        self.opt = tf.train.AdagradOptimizer(self.lr)\n",
    "    else:\n",
    "        raise KeyError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#方式一：opt.compute_gradients(self.loss)\n",
    "grads_vars = self.opt.compute_gradients(self.loss) #根据损失，计算梯度，默认也是针对trainable_variables\n",
    "#返回的是 A list of (gradient, variable) pairs.故而grads_vars是一个列表\n",
    "capped_grads_vars = [[tf.clip_by_value(g, -self.config[\"clip\"], self.config[\"clip\"]), v]\n",
    "                     for g, v in grads_vars] #截断梯度，前后都截断，防止梯度爆炸和消失，纯数值截断，比如大于5的梯度都设置为5，比-5小的设置为-5\n",
    "#这里选择的是前后都截断，也可以选择下面的方式，即按一定比例变小\n",
    "self.train_op = self.opt.apply_gradients(capped_grads_vars, self.global_step) #更新参数\n",
    "#注意，这里需要把train_op放sess.run里面，才会去更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#方式二：tf.gradients(loss, params) \n",
    "params = tf.trainable_variables()\n",
    "gradients = tf.gradients(loss, params) \n",
    "self.updates = []\n",
    "#通过损失对需要更新的参数进行求梯度，更新选择的参数。\n",
    "#tf.gradients最重要的即两个参数，一个即ys，这里即loss，另一个即xs，这里即要更新的params，即用ys对xs进行求偏导\n",
    "clipped_gradients, norm = tf.clip_by_global_norm( gradients, clip_norm) #t_list[i] * clip_norm / max(global_norm, clip_norm)\n",
    "                                                                        #global_norm = sqrt(sum([l2norm(t)**2 for t in t_list]))\n",
    "self.gradient_norms.append(norm)\n",
    "self.updates.append(opt.apply_gradients(zip(clipped_gradients, params))) #反向传播，更新参数\n",
    "#同样的，这里的updates也要放在sess.run里面，即只有updates被运行了，参数才会被更新\n",
    "#当然，最后一句话换成以下的也可以，同样是为了将梯度更新到参数，这里因为是多组，所以用一个列表接收\n",
    "#self.train_op = opt.apply_gradients(zip(clipped_gradients, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#另一种梯度截断tf.clip_by_norm\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, beta1=0.5)\n",
    "grads = optimizer.compute_gradients(loss)\n",
    "for i, (g, v) in enumerate(grads):\n",
    "    if g is not None:\n",
    "        grads[i] = (tf.clip_by_norm(g, 5), v)  # 即对每个g做以下处理：`g * clip_norm / l2norm(g)`,相比global_norm少了一个max\n",
    "train_op = optimizer.apply_gradients(grads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
