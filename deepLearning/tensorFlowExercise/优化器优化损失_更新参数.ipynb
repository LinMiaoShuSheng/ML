{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#一般的写法，即不做梯度截断时的正常优化\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-3) #定义优化器，设定学习率\n",
    "grads_and_vars=optimizer.compute_gradients(loss) #对loss求梯度\n",
    "tr_op_set = optimizer.apply_gradients(grads_and_vars, global_step=global_step) #更新参数\n",
    "\n",
    "#因为也没有做梯度截断操作，故而上面三句也可以直接写成一句话\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#做梯度截断时候的方式：\n",
    "#方式一：tf.clip_by_value  #纯数值截断，即自定义上下界，即同时防止爆炸和消失\n",
    "#方式二：tf.clip_by_global_norm  #这个严格来说不是截断，是按一定比例变小，主要是为了防止梯度爆炸\n",
    "#方式三：tf.clip_by_norm  #这个和第二种方式一个性质，也是让梯度变小，和上面的区别是固定地除上梯度g的l2正则\n",
    "\n",
    "#对于计算梯度\n",
    "#则即可以是opt.compute_gradients(loss,params) #默认params都是trainable的变量\n",
    "#也可以是tf.gradients(loss, params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#方式一：\n",
    "grads_vars = self.opt.compute_gradients(self.loss) #根据损失，计算梯度，默认也是针对trainable_variables\n",
    "#返回的是 A list of (gradient, variable) pairs.故而grads_vars是一个列表\n",
    "capped_grads_vars = [[tf.clip_by_value(g, -self.config[\"clip\"], self.config[\"clip\"]), v]\n",
    "                     for g, v in grads_vars] #截断梯度，前后都截断，防止梯度爆炸和消失，纯数值截断，比如大于5的梯度都设置为5，比-5小的设置为-5\n",
    "#这里选择的是前后都截断，也可以选择下面的方式，即按一定比例变小\n",
    "self.train_op = self.opt.apply_gradients(capped_grads_vars, self.global_step) #更新参数\n",
    "#注意，这里需要把train_op放sess.run里面，才会去更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#方式二：\n",
    "params = tf.trainable_variables()\n",
    "gradients = tf.gradients(loss, params) \n",
    "self.updates = []\n",
    "#通过损失对需要更新的参数进行求梯度，更新选择的参数。\n",
    "#tf.gradients最重要的即两个参数，一个即ys，这里即loss，另一个即xs，这里即要更新的params，即用ys对xs进行求偏导\n",
    "clipped_gradients, norm = tf.clip_by_global_norm( gradients, clip_norm) #t_list[i] * clip_norm / max(global_norm, clip_norm)\n",
    "                                                                        #global_norm = sqrt(sum([l2norm(t)**2 for t in t_list]))\n",
    "self.gradient_norms.append(norm)\n",
    "self.updates.append(opt.apply_gradients(zip(clipped_gradients, params))) #反向传播，更新参数\n",
    "#同样的，这里的updates也要放在sess.run里面，即只有updates被运行了，参数才会被更新\n",
    "#当然，最后一句话换成以下的也可以，同样是为了将梯度更新到参数，这里因为是多组，所以用一个列表接收\n",
    "#self.train_op = opt.apply_gradients(zip(clipped_gradients, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#方式三：\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, beta1=0.5)\n",
    "grads = optimizer.compute_gradients(loss)\n",
    "for i, (g, v) in enumerate(grads):\n",
    "    if g is not None:\n",
    "        grads[i] = (tf.clip_by_norm(g, 5), v)  # 即对每个g做以下处理：`g * clip_norm / l2norm(g)`,相比global_norm少了一个max\n",
    "train_op = optimizer.apply_gradients(grads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
