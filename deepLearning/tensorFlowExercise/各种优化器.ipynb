{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# =========================================================================== #\n",
    "# Optimization Flags.\n",
    "# =========================================================================== #\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'weight_decay', 0.00004, 'The weight decay on the model weights.')\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'optimizer', 'rmsprop',\n",
    "    'The name of the optimizer, one of \"adadelta\", \"adagrad\", \"adam\",'\n",
    "    '\"ftrl\", \"momentum\", \"sgd\" or \"rmsprop\".')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'adadelta_rho', 0.95,\n",
    "    'The decay rate for adadelta.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'adagrad_initial_accumulator_value', 0.1,\n",
    "    'Starting value for the AdaGrad accumulators.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'adam_beta1', 0.9,\n",
    "    'The exponential decay rate for the 1st moment estimates.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'adam_beta2', 0.999,\n",
    "    'The exponential decay rate for the 2nd moment estimates.')\n",
    "tf.app.flags.DEFINE_float('opt_epsilon', 1.0, 'Epsilon term for the optimizer.')\n",
    "tf.app.flags.DEFINE_float('ftrl_learning_rate_power', -0.5,\n",
    "                          'The learning rate power.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'ftrl_initial_accumulator_value', 0.1,\n",
    "    'Starting value for the FTRL accumulators.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'ftrl_l1', 0.0, 'The FTRL l1 regularization strength.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'ftrl_l2', 0.0, 'The FTRL l2 regularization strength.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'momentum', 0.9,\n",
    "    'The momentum for the MomentumOptimizer and RMSPropOptimizer.')\n",
    "tf.app.flags.DEFINE_float('rmsprop_momentum', 0.9, 'Momentum.')\n",
    "tf.app.flags.DEFINE_float('rmsprop_decay', 0.9, 'Decay term for RMSProp.')\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_optimizer(flags, learning_rate):\n",
    "    \"\"\"Configures the optimizer used for training.\n",
    "    Args:\n",
    "      learning_rate: A scalar or `Tensor` learning rate.\n",
    "    Returns:\n",
    "      An instance of an optimizer.\n",
    "    \"\"\"\n",
    "    if flags.optimizer == 'adadelta':\n",
    "        optimizer = tf.train.AdadeltaOptimizer(\n",
    "            learning_rate,\n",
    "            rho=flags.adadelta_rho,\n",
    "            epsilon=flags.opt_epsilon)\n",
    "    elif flags.optimizer == 'adagrad':\n",
    "        optimizer = tf.train.AdagradOptimizer(\n",
    "            learning_rate,\n",
    "            initial_accumulator_value=flags.adagrad_initial_accumulator_value)\n",
    "    elif flags.optimizer == 'adam':\n",
    "        optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate,\n",
    "            beta1=flags.adam_beta1,\n",
    "            beta2=flags.adam_beta2,\n",
    "            epsilon=flags.opt_epsilon)\n",
    "    elif flags.optimizer == 'ftrl':\n",
    "        optimizer = tf.train.FtrlOptimizer(\n",
    "            learning_rate,\n",
    "            learning_rate_power=flags.ftrl_learning_rate_power,\n",
    "            initial_accumulator_value=flags.ftrl_initial_accumulator_value,\n",
    "            l1_regularization_strength=flags.ftrl_l1,\n",
    "            l2_regularization_strength=flags.ftrl_l2)\n",
    "    elif flags.optimizer == 'momentum':\n",
    "        optimizer = tf.train.MomentumOptimizer(\n",
    "            learning_rate,\n",
    "            momentum=flags.momentum,\n",
    "            name='Momentum')\n",
    "    elif flags.optimizer == 'rmsprop':\n",
    "        optimizer = tf.train.RMSPropOptimizer(\n",
    "            learning_rate,\n",
    "            decay=flags.rmsprop_decay,\n",
    "            momentum=flags.rmsprop_momentum,\n",
    "            epsilon=flags.opt_epsilon)\n",
    "    elif flags.optimizer == 'sgd':\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    else:\n",
    "        raise ValueError('Optimizer [%s] was not recognized', flags.optimizer)\n",
    "    return optimizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
